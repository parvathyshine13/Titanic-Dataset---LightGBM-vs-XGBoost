# üö¢ Titanic Dataset - LightGBM vs XGBoost

- Performed exploratory data analysis (EDA) on the Titanic dataset to understand feature distributions and check for missing values.
- Handled missing data through imputation and removed irrelevant columns like 'Cabin', 'Name', and 'Ticket'.
- Encoded categorical variables such as 'Sex' and 'Embarked' using one-hot or label encoding.
- Split the cleaned dataset into training and testing sets for model evaluation.
- Built classification models using both LightGBM and XGBoost algorithms.
- Evaluated the models using metrics like accuracy, precision, recall, and F1-score.
- Applied cross-validation and hyperparameter tuning to optimize performance.
- Compared the performance of LightGBM and XGBoost based on evaluation metrics and training time.
- Visualized feature importances to interpret the most influential variables in predicting survival.

---

## üõ†Ô∏è Tools Used
- Python, Pandas, NumPy
- Scikit-learn, XGBoost, LightGBM
- Matplotlib, Seaborn
